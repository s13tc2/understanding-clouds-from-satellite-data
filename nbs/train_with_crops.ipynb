{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629757d8-936c-4b0e-99db-f102257b8b86",
   "metadata": {},
   "source": [
    "# Train With Crops\n",
    "\n",
    "This notebook is an attempt to convert Chris's excellent notebook [here](https://www.kaggle.com/code/cdeotte/train-with-crops-lb-0-63) from Tensorflow/Keras to PyTorch. Some code cells that did not use Tensorflow/Keras were re-used with no edits made.\n",
    "\n",
    "**Everything is based on Chris's notebook.** <br>\n",
    "All credit belongs to the original author!\n",
    "\n",
    "## Objective\n",
    "\n",
    "- In this kernel we use a trick that we learned in Kaggle's Steel Competition. Since segmentation neural networks are all convolutions, you can train with one input size and predict with a different input size (explained in detail [here][1]). First we will resize all training and test images into size `700x1050` from their original `1400x2100`. Next we will train with random `352x512` crops and then feed full test images (`700x1050` images) into our network and get full segmentation masks!\n",
    "\n",
    "[1]: https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114321"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ddcc7c-3b84-407e-999b-a62b70f3ca3f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89312e-6f98-4b5b-884f-afe90c1c57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import timm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image \n",
    "import albumentations\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt, time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "\n",
    "from utils import mask2rleX, rle2maskX, mask2contour, clean, dice_coef6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab02c9-5940-4aff-8979-24df1f269373",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfd0d3-eec5-4564-a0f4-f81dc2111bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    lr = 1e-3\n",
    "    n_epochs = 4\n",
    "    batch_size = 8\n",
    "    n_workers = 4\n",
    "    n_folds = 3\n",
    "    seed = 42\n",
    "    n_classes = 4\n",
    "    pred_batch_size = 2\n",
    "    pred_width = 1024\n",
    "    pred_height = 672\n",
    "    crop_width = 512\n",
    "    crop_height = 352\n",
    "    shrink = 2\n",
    "    orig_width = 2100\n",
    "    orig_height = 1400\n",
    "    train_img_folder = '../data/understanding_clouds/train_images'\n",
    "    test_img_folder = '../data/understanding_clouds/test_images'\n",
    "    net = 'resnet18'\n",
    "    base_model_folder = './drive/MyDrive/understanding_clouds_models'\n",
    "    train_crops_folder = './drive/MyDrive/understanding_clouds_models/train_with_crops'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43aa27b-85d9-44dc-8c80-4f7cc7bef7a5",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ce618-4875-4a9a-b90f-5ba31aceb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/understanding_clouds/sample_submission.csv')\n",
    "sub['Image'] = sub['Image_Label'].map(lambda x: x.split('.')[0])\n",
    "\n",
    "PATH = '../data/understanding_clouds/train_images/'\n",
    "train = pd.read_csv('../data/understanding_clouds/train.csv')\n",
    "train['Image'] = train['Image_Label'].map(lambda x: x.split('.')[0])\n",
    "train['Label'] = train['Image_Label'].map(lambda x: x.split('_')[1])\n",
    "train2 = pd.DataFrame({'Image':train['Image'][::4]})\n",
    "train2['e1'] = train['EncodedPixels'][::4].values\n",
    "train2['e2'] = train['EncodedPixels'][1::4].values\n",
    "train2['e3'] = train['EncodedPixels'][2::4].values\n",
    "train2['e4'] = train['EncodedPixels'][3::4].values\n",
    "train2.set_index('Image',inplace=True,drop=True)\n",
    "train2.fillna('',inplace=True); train2.head()\n",
    "train2[['d1','d2','d3','d4']] = (train2[['e1','e2','e3','e4']]!='').astype('int8')\n",
    "train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d65d6d-73eb-413f-bc17-701797c67e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = train2.iloc[:, 4:8].columns.tolist()\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78f83e-129d-4e2a-add6-622a59bfbb49",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "This data generator outputs random crops of size 352x512. These crops are taken from the original 1400x2100 training images after they are resized to 700x1050 pixels. The masks are cropped to match the image crops. Also we have horizontal and vertical flip augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b42712-f349-4208-934f-a9b6b505fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, mode='train', height=cfg.crop_height, width=cfg.crop_width, shrink=cfg.shrink):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train' or self.mode == 'display':\n",
    "            self.labels = self.df[target_cols].values\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.shrink = shrink\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train': \n",
    "            msk = np.empty((self.height,self.width,4),dtype=np.int8)\n",
    "            crp = np.zeros((2),dtype=np.int16)\n",
    "\n",
    "            a = np.random.randint(0,cfg.orig_width//self.shrink-self.width+1)\n",
    "            b = np.random.randint(0,cfg.orig_height//self.shrink-self.height+1)\n",
    "            \n",
    "            image_id = self.df.index[index]\n",
    "            path = f'{cfg.train_img_folder}/{image_id}.jpg'\n",
    "\n",
    "            if os.path.exists(path):\n",
    "                img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)              \n",
    "                img = cv2.resize(img,(cfg.orig_width//self.shrink,cfg.orig_height//self.shrink),interpolation=cv2.INTER_AREA)\n",
    "                img = img[b:self.height+b,a:self.width+a]\n",
    "\n",
    "                for j in range(1, 5):\n",
    "                    rle = self.df['e'+str(j)][index]\n",
    "                    mask = rle2maskX(rle, shrink=self.shrink)\n",
    "                    msk[:,:,j-1] = mask[b:self.height+b,a:self.width+a]\n",
    "\n",
    "                if self.transform is not None:\n",
    "                    res = self.transform(image=img, mask=msk)\n",
    "                    img = res['image']\n",
    "                    msk = res['mask']\n",
    "\n",
    "                img = img.astype(np.float32)\n",
    "                img = img.transpose(2,0,1)\n",
    "                \n",
    "                return torch.tensor(img), torch.tensor(msk)\n",
    "        \n",
    "        elif self.mode == 'display':\n",
    "            msk = np.empty((self.height,self.width,4),dtype=np.int8)\n",
    "            crp = np.zeros((2),dtype=np.int16)\n",
    "\n",
    "            a = np.random.randint(0,cfg.orig_width//self.shrink-self.width+1)\n",
    "            b = np.random.randint(0,cfg.orig_height//self.shrink-self.height+1)\n",
    "            \n",
    "            crp[0] = a; crp[1] = b\n",
    "            \n",
    "            image_id = self.df.index[index]\n",
    "            path = f'{cfg.train_img_folder}/{image_id}.jpg'\n",
    "\n",
    "            if os.path.exists(path):\n",
    "                img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)              \n",
    "                img = cv2.resize(img,(cfg.orig_width//self.shrink,cfg.orig_height//self.shrink),interpolation=cv2.INTER_AREA)\n",
    "                img = img[b:self.height+b,a:self.width+a]\n",
    "\n",
    "                for j in range(1, 5):\n",
    "                    rle = self.df['e'+str(j)][index]\n",
    "                    mask = rle2maskX(rle, shrink=self.shrink)\n",
    "                    msk[:,:,j-1] = mask[b:self.height+b,a:self.width+a]\n",
    "\n",
    "                if self.transform is not None:\n",
    "                    res = self.transform(image=img, mask=msk)\n",
    "                    img = res['image']\n",
    "                    msk = res['mask']\n",
    "\n",
    "                img = img.astype(np.float32)\n",
    "                img = img.transpose(2,0,1)\n",
    "\n",
    "                \n",
    "                return torch.tensor(img), torch.tensor(msk), crp\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            a = (2100//self.shrink-self.width)//2\n",
    "            b = (1400//self.shrink-self.height)//2\n",
    "            \n",
    "            image_id = self.df.index[index]\n",
    "            path = f'{cfg.test_img_folder}/{image_id}.jpg'\n",
    "            \n",
    "            if os.path.exists(path):\n",
    "                img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)              \n",
    "                img = cv2.resize(img,(cfg.orig_width//self.shrink,cfg.orig_height//self.shrink),interpolation=cv2.INTER_AREA)\n",
    "                img = img[b:self.height+b,a:self.width+a]\n",
    "\n",
    "                img = img.astype(np.float32)\n",
    "                img = img.transpose(2,0,1)\n",
    "                \n",
    "                return torch.tensor(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1b8fe-01fc-4c4f-8bf3-7d3fb0196cc6",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03777ed3-47be-40f3-85ef-a6b6aedbd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = albumentations.Compose([\n",
    "  albumentations.HorizontalFlip(p=0.5),\n",
    "  albumentations.VerticalFlip(p=0.5),\n",
    "  albumentations.Normalize(),\n",
    "])\n",
    "\n",
    "transforms_valid = albumentations.Compose([\n",
    "  albumentations.Normalize(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ef1f7-0651-4bfe-a48f-c76fa0ebb38b",
   "metadata": {},
   "source": [
    "Below we display examples. The image on the left is the original image. The yellow rectangle is an original mask. The black rectangle is a random crop. The image on the right is the random crop outputted by the data generator. Notice how the original mask is cropped too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878372f-556c-42cf-9c9c-4cadff947111",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CloudDataset(train2[:8], mode='display', transform=transforms_train)\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdfc46-58dc-4049-8b5b-f326e78c124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['Fish','Flower','Gravel','Sugar']\n",
    "\n",
    "for step, batch in enumerate(train_loader):\n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    # RANDOMLY PICK CLOUD TYPE TO DISPLAY FROM NON-EMPTY MASKS\n",
    "    idx = np.argwhere( train2.loc[train2.index[step],['d1','d2','d3','d4']].values==1 ).flatten()\n",
    "    d = np.random.choice(idx)+1\n",
    "\n",
    "    # DISPLAY ORIGINAL\n",
    "    img = Image.open(PATH+train2.index[step]+'.jpg'); img=np.array(img)\n",
    "    mask = rle2maskX( train2.loc[train2.index[step],'e'+str(d)] )\n",
    "    contour = mask2contour( mask,10 )\n",
    "    img[contour==1,:2]=255\n",
    "    diff = np.ones((1400,2100,3),dtype=np.int)*255-img.astype(int)\n",
    "    img=img.astype(int); img[mask==1,:] += diff[mask==1,:]//6\n",
    "    mask = np.zeros((1400,2100))\n",
    "    a = batch[2][0,1]*2\n",
    "    b = batch[2][0,0]*2\n",
    "    mask[a:a+2*352,b:b+2*512]=1\n",
    "    mask = mask2contour(mask,20)\n",
    "    img[mask==1,:]=0\n",
    "    plt.subplot(1,2,1); \n",
    "    plt.title('Original - '+train2.index[step]+'.jpg - '+types[d-1])\n",
    "    plt.imshow(img);\n",
    "\n",
    "    # DISPLAY RANDOM CROP\n",
    "    img = batch[0].cpu().detach().numpy()\n",
    "    mask = batch[1].cpu().detach().numpy()\n",
    "    img = img[0].transpose(1,2,0)\n",
    "    mask = mask[0][:,:,d-1]\n",
    "    contour = mask2contour( mask )\n",
    "    img[contour==1,:2]=255\n",
    "    diff = np.ones((352,512,3),dtype=np.int)*img\n",
    "    img=img; img[mask==1,:] += diff[mask==1,:]//6\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Training Crop - '+train2.index[step]+'.jpg - '+types[d-1])\n",
    "    plt.imshow( img ); \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f5c71-1dd2-4e56-9c1b-4a5626986a77",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd85208-cc7b-460e-8658-c41b08845b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_metric(probabilities: torch.Tensor,\n",
    "                     truth: torch.Tensor,\n",
    "                     treshold: float = 0.5,\n",
    "                     eps: float = 1e-9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Dice score for data batch.\n",
    "    Params:\n",
    "        probobilities: model outputs after activation function.\n",
    "        truth: truth values.\n",
    "        threshold: threshold for probabilities.\n",
    "        eps: additive to refine the estimate.\n",
    "        Returns: dice score aka f1.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    num = probabilities.shape[0]\n",
    "    predictions = (probabilities >= treshold).float()\n",
    "    assert(predictions.shape == truth.shape)\n",
    "    for i in range(num):\n",
    "        prediction = predictions[i]\n",
    "        truth_ = truth[i]\n",
    "        intersection = 2.0 * (truth_ * prediction).sum()\n",
    "        union = truth_.sum() + prediction.sum()\n",
    "        if truth_.sum() == 0 and prediction.sum() == 0:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append((intersection + eps) / union)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32ead8-a72b-49a5-a9e2-64c6e22da5f2",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e99a3-e019-4564-8c89-a6e6685716da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    smooth = 1.\n",
    "\n",
    "    iflat = input.reshape(-1)\n",
    "    tflat = target.reshape(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a180794-6a18-4d50-bf96-dc2cba9ee783",
   "metadata": {},
   "source": [
    "# Train and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ffae1-9472-4684-bdb8-14fde365ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(model, loader, optimizer, loss_func, dice_coef_metric, device):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    dice = []\n",
    "\n",
    "    bar = tqdm(enumerate(loader), total=len(loader))\n",
    "\n",
    "    for step, (data, targets) in bar:\n",
    "        data = data.to(device)\n",
    "        targets = targets.permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        loss = loss_func(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        dice_scores = dice_coef_metric(outputs.detach().cpu(), targets.detach().cpu(), 0.5)\n",
    "    \n",
    "        losses.append(loss.item())\n",
    "        dice.append(dice_scores)\n",
    "        smooth_loss = np.mean(losses[-30:])\n",
    "        bar.set_description(f'loss: {loss.item():.5f}, smth: {smooth_loss:.5f}, dice: {dice_scores:.5f}')\n",
    "\n",
    "    return np.array(losses).mean(), np.array(dice).mean()\n",
    "\n",
    "\n",
    "def valid_func(model, loader, loss_func, dice_coef_metric, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    dice = []\n",
    "\n",
    "    bar = tqdm(enumerate(loader), total=len(loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (data, targets) in bar:\n",
    "            data = data.to(device)\n",
    "            targets = targets.permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "            outputs = model(img)\n",
    "            loss = loss_func(outputs, msk)\n",
    "\n",
    "            dice_scores = dice_coef_metric(outputs.detach().cpu(), targets.detach().cpu(), 0.5)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            dice.append(dice_scores)\n",
    "            smooth_loss = np.mean(losses[-30:])\n",
    "            bar.set_description(f'loss: {loss.item():.5f}, smth: {smooth_loss:.5f}, dice: {dice_scores:.5f}')\n",
    "\n",
    "    return np.array(losses).mean(), np.array(dice).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80181ec-12e7-485f-b5f3-2893d5482b84",
   "metadata": {},
   "source": [
    "# Create K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7ae61-034d-4aca-8a2c-f08d2eaf89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(cfg.n_folds, shuffle=True, random_state=cfg.seed)\n",
    "train2[\"fold\"] = -1\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(train2)):\n",
    "    train2.loc[train2.index[valid_idx], \"fold\"] = i\n",
    "train2.to_csv('folds.csv', index=False)\n",
    "train2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52138f81-d725-4e1c-848a-38fbbeafb583",
   "metadata": {},
   "source": [
    "# Build Segmentation Model\n",
    "We will build a segmentation model using Qubvel's Pytorch Segmentation models [here][1]. Our architecture will be FPN (feature pyramid network) and our backbone will be ResNet18. We will use Dice loss and Adam optimizer with learning rate 1e-3. Our metric will be Dice coef.\n",
    "\n",
    "[1]: https://github.com/qubvel/segmentation_models.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011c37c-afc4-443a-9f66-fb55a06d603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = smp.FPN(\n",
    "    encoder_name=cfg.net,       \n",
    "    encoder_weights=\"imagenet\",     \n",
    "    classes=cfg.n_classes, \n",
    "    activation='sigmoid'                     \n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, cfg.n_epochs)\n",
    "loss_fn = dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa7243-b48d-4017-86c5-ff3e980e4747",
   "metadata": {},
   "source": [
    "# Train Segmentation Model on Crops\n",
    "We will train with `352x512` crops, `batch_size=8`, and use 3-Fold validation. During training we predict OOF. We predict `test.csv` afterwards by saving the 3 models from the 3 folds. We will train each fold for 4 epochs. In our predicted oof masks, all pixel probabilities over 0.4 will be converted to 1 and less than 0.4 to 0. Any mask with fewer than `4*20000` pixels (predicted on `700x1050` image) will be regarded as no mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030eebd-d92b-4489-a9da-da8447e97c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.empty_like(train2[['e1','e2','e3','e4']].values)\n",
    "\n",
    "for fold in range(cfg.n_folds):\n",
    "    train_idx = np.where((train2[\"fold\"] != fold))[0]\n",
    "    valid_idx = np.where((train2[\"fold\"] == fold))[0]\n",
    "\n",
    "    print(); print('#'*10,'FOLD',fold,'#'*10)\n",
    "    print('Train on',len(train_idx),'Validate on',len(valid_idx))\n",
    "\n",
    "    train_ds = CloudDataset(train2.loc[train2.index[train_idx]], transform=transforms_train)\n",
    "    valid_ds = CloudDataset(train2.loc[train2.index[valid_idx]], transform=transforms_valid)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, num_workers=cfg.n_workers, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=cfg.batch_size, num_workers=cfg.n_workers, shuffle=False)\n",
    "\n",
    "    best_dice = 0.0\n",
    "\n",
    "    for epoch in range(1, cfg.n_epochs+1):\n",
    "        loss_train, dice_train = train_func(model, train_loader, optimizer, loss_fn, dice_coef_metric, device)\n",
    "        scheduler.step(epoch)\n",
    "        loss_valid, dice_valid = valid_func(model, valid_loader, loss_fn, dice_coef_metric, device)\n",
    "\n",
    "        content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, loss_train: {loss_train:.5f}, loss_valid: {loss_valid:.5f}, dice: {dice_valid:.6f}.'\n",
    "        print(content)\n",
    "\n",
    "        if dice_valid > best_dice:\n",
    "            print(f'accuracy ({best_dice:.6f} --> {dice_valid:.6f}). Saving model ...')\n",
    "            torch.save(model.state_dict(), f'{cfg.train_crops_folder}/{cfg.net}_best_dice_fold{fold}.pth')\n",
    "            best_dice = dice_valid   \n",
    "  \n",
    "    # PREDICT OOF\n",
    "    print('Predict OOF: ',end='')\n",
    "    valid_ds = CloudDataset(train2.loc[train2.index[valid_idx]], width=cfg.pred_width, height=cfg.pred_height, mode='predict', transform=transforms_valid)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=cfg.pred_batch_size, num_workers=cfg.n_workers, shuffle=False)\n",
    "    model.eval()\n",
    "\n",
    "    bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (data, targets) in bar:\n",
    "            data = data.to(device)\n",
    "            targets = targets.permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            out = outputs.permute(0, 2, 3, 1)\n",
    "            for j in range(out.shape[0]):\n",
    "                for i in range(out.shape[-1]):\n",
    "                    mask = out[j,:,:,i]\n",
    "                    mask = mask.cpu().detach().numpy()\n",
    "                    mask = mask > 0.4\n",
    "                    rle =''\n",
    "                    if np.sum(mask)>4*20000:\n",
    "                        rle = mask2rleX(mask)\n",
    "                    oof[valid_idx[2*step+j],i] = rle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a931dfd-f60e-46b4-8ace-df82338fb1bd",
   "metadata": {},
   "source": [
    "# Save OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06293e2a-b8bb-4a08-8df0-e30b91df00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{cfg.base_model_folder}/oof_seg_preds_3fold.npy', oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483db81e-8ce9-4cac-b22c-da82cbdea690",
   "metadata": {},
   "source": [
    "# Evaluate Segmentation Model using OOF\n",
    "We will evaluate OOF using Kaggle's Dice metric. For post processing, we will remove any contiguous piece of predicted mask with fewer than 20000 pixels (predicted on `350x525` image). And we will remove any mask with less than 0.5 probability as determined by our cloud classifer from our previous notebook [here][1]\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/cloud-bounding-boxes-cv-0-58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2423c3-5bc6-4e59-ab9a-ec23a4633cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CLASSIFICATION PREDICTIONS FROM PREVIOUS KERNEL\n",
    "# https://www.kaggle.com/cdeotte/cloud-bounding-boxes-cv-0-58\n",
    "for k in range(1,5): train2['o'+str(k)] = 0\n",
    "train2[['o1','o2','o3','o4']] = np.load(f'{cfg.base_model_folder}/oof.npy')[:len(train2),]\n",
    "\n",
    "# LOAD OOF SEGMENTATION PREDICTIONS FROM 3-FOLD ABOVE\n",
    "for k in range(1,5): train2['ee'+str(k)] = ''\n",
    "train2[['ee1','ee2','ee3','ee4']] = oof\n",
    "for k in range(1,5): train2['ee'+str(k)] = train2['ee'+str(k)].map(clean)\n",
    "\n",
    "# COMPUTE KAGGLE DICE\n",
    "th = [0.5,0.5,0.5,0.5]\n",
    "for k in range(1,5):\n",
    "    train2['ss'+str(k)] = train2.apply(lambda x:dice_coef6(x['e'+str(k)],x['o'+str(k)],x['ee'+str(k)],th[k-1]),axis=1)\n",
    "    dice = np.round( train2['ss'+str(k)].mean(),3 )\n",
    "    print(types[k-1],': Kaggle Dice =',dice)\n",
    "dice = np.round( np.mean( train2[['ss1','ss2','ss3','ss4']].values ),3 )\n",
    "print('Overall : Kaggle Dice =',dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a8d076-fcd3-4b36-bce4-73c81a40ee23",
   "metadata": {},
   "source": [
    "# View OOF Examples\n",
    "Below yellow outlines are true masks and blue outlines (with shaded insides) are predicted masks. The Dice score for each predicted mask is displayed above each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb851aea-15ae-4b8b-9762-7bbe26f35536",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(1,5):\n",
    "    print('#'*27); print('#'*5,types[d-1].upper(),'CLOUDS','#'*7); print('#'*27)\n",
    "    plt.figure(figsize=(20,15)); k=0\n",
    "    for kk in range(9):\n",
    "        plt.subplot(3,3,kk+1)\n",
    "        while (train2.loc[train2.index[k],'e'+str(d)]==''): k += 1\n",
    "        f = train2.index[k]+'.jpg'\n",
    "        img = Image.open(PATH+f); img = img.resize((525,350)); img = np.array(img)\n",
    "        rle1 = train2.loc[train2.index[k],'e'+str(d)]; mask = rle2maskX(rle1,shrink=4)\n",
    "        contour = mask2contour(mask,5); img[contour==1,:2] = 255\n",
    "        rle2 = train2.loc[train2.index[k],'ee'+str(d)]; mask = rle2maskX(rle2,shape=(525,350))\n",
    "        contour = mask2contour(mask,5); img[contour==1,2] = 255\n",
    "        diff = np.ones((350,525,3),dtype=np.int)*255-img\n",
    "        img=img.astype(int); img[mask==1,:] += diff[mask==1,:]//4\n",
    "        dice = np.round( dice_coef6(rle1,1,rle2,0),3 )\n",
    "        plt.title(f+'  Dice = '+str(dice)+'   Yellow true, Blue predicted')\n",
    "        plt.imshow(img); k += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bbcc9-73dc-4928-8b82-fb0f782cbb11",
   "metadata": {},
   "source": [
    "# Predict Test Images\n",
    "We load the models from our 3 folds above and use them to predict `test.csv`. Note that we must choose an input size that is divisble by 32. Therefore we choose `672x1024` instead of `700x1050`. We segment the middle of each test image ignoring the 14 pixel wide border around the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98cf7de-eee5-453b-b03e-6140ac6d4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = pd.DataFrame({'Image':sub['Image'][::4]}).reset_index(drop=True)\n",
    "sub2.set_index('Image',inplace=True,drop=True)\n",
    "sub2.fillna('',inplace=True); \n",
    "sub2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358c220-b948-4f5a-9db5-eda48a0e9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing masks for',len(sub)//4,'test images with 3 models'); sub.EncodedPixels = ''\n",
    "test_ds = TestCloudDataset(sub2, width=cfg.pred_width, height=cfg.pred_height, mode='predict', transform=transforms_valid)\n",
    "test_loader = DataLoader(test_ds, batch_size=cfg.pred_batch_size, num_workers=cfg.n_workers, shuffle=False)\n",
    "\n",
    "model = smp.FPN(\n",
    "    encoder_name=\"resnet18\",       \n",
    "    encoder_weights=\"imagenet\",     \n",
    "    classes=cfg.n_classes, \n",
    "    activation='sigmoid'                     \n",
    ").to(device)\n",
    "\n",
    "m1 = model\n",
    "m1.load_state_dict(torch.load(f'{cfg.train_crops_folder}/{cfg.net}_best_dice_fold0.pth'))\n",
    "m1 = m1.to(device)\n",
    "\n",
    "m2 = model\n",
    "m2.load_state_dict(torch.load(f'{cfg.train_crops_folder}/{cfg.net}_best_dice_fold1.pth'))\n",
    "m2 = m2.to(device)\n",
    "\n",
    "m3 = model\n",
    "m3.load_state_dict(torch.load(f'{cfg.train_crops_folder}/{cfg.net}_best_dice_fold2.pth'))\n",
    "m3 = m3.to(device)\n",
    "\n",
    "m1.eval()\n",
    "m2.eval()\n",
    "m3.eval()\n",
    "\n",
    "bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "with torch.no_grad():\n",
    "    for step, data in bar:\n",
    "        data = data.to(device)\n",
    "\n",
    "        outputs = m1(data)\n",
    "        outputs += m2(data)\n",
    "        outputs += m3(data)\n",
    "\n",
    "        outputs /= 3.0\n",
    "        out = outputs.permute(0, 2, 3, 1)\n",
    "        for j in range(out.shape[0]):\n",
    "            for i in range(out.shape[-1]):\n",
    "                mask = out[j,:,:,i]\n",
    "                mask = mask.cpu().detach().numpy()\n",
    "                mask = mask > 0.4\n",
    "                rle = ''\n",
    "                if np.sum(mask)>4*20000:\n",
    "                    rle = mask2rleX(mask)\n",
    "                sub.iloc[4*(2*step+j)+i, 1] = rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f005b85-7b23-4d4a-bb17-0557ff64a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CLASSIFICATION PREDICTIONS FROM PREVIOUS KERNEL\n",
    "# https://www.kaggle.com/cdeotte/cloud-bounding-boxes-cv-0-58\n",
    "sub['p'] = np.load(f'{cfg.base_model_folder}/preds.npy').reshape((-1))[:len(sub)]\n",
    "sub.loc[sub.p<0.5,'EncodedPixels'] = ''\n",
    "\n",
    "sub.EncodedPixels = sub.EncodedPixels.map(clean)\n",
    "sub[['Image_Label','EncodedPixels']].to_csv('submission.csv',index=False)\n",
    "sub.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
